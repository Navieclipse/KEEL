<method>

	<name>FURIA</name>

	<reference>  

		<ref>J. Huhn and E. Hullermeier. FURIA: an algorithm for unordered fuzzy rule induction (2009) Data Mining and Knowledge Discovery, 19 (3), pp. 293-319. </ref>

	</reference>

	<generalDescription>  

		<type>Fuzzy Rule Based Classification System with Hyperrectangles</type>

		<objective>To extract a compact set of good fuzzy rules from numerical data</objective>

		<howWork>
		The FURIA algorithm is a fuzzy rule learner based on the RIPPER implementation. The main difference between FURIA and RIPPER is that FURIA makes no use of default rules. Furthermore FURIA has a changed pruning procedure, which means that the pruning during the IREP* runs was deactivated permanently. It was found out experimentally that this improved the classification rate
		
		Initialize RS = {}, and for each of both class DO: 
			
			1. Building stage:
				Repeat 1.1 until the description length (DL) of the ruleset and examples is 64 bits greater than the smallest DL met so far, or there are no positive examples, or the error rate >= 50%. 
			
				1.1. Grow phase:
				Grow one rule by greedily adding antecedents (or conditions) to the rule until the rule is perfect (i.e. 100% accurate).  The procedure tries every possible value of each attribute and selects the condition with highest information gain: p(log(p/t)-log(P/T)).
			
			2. Optimization stage:
				after generating the initial ruleset {Ri}, generate and prune two variants of each rule Ri from randomized data using procedure 1.1 and X.1. But one variant is generated from an empty rule while the other is generated by greedily adding antecedents to the original rule. 
				Moreover, the pruning metric used here is (TP+TN)/(P+N). Then the smallest possible DL for each variant and the original rule is computed.  The variant with the minimal DL is selected as the final representative of Ri in the ruleset.
				After all the rules in {Ri} have been examined and if there are still residual positives, more rules are generated based on the residual positives using Building Stage again. 
			3. Delete the rules from the ruleset that would increase the DL of the whole ruleset if it were in it. and add resultant ruleset to RS. 
		ENDDO
		
		Fuzzification of RS:
		For each rule r in every ruleset in RS DO
		4. Fuzzification of antecedents: 
		Apply greedy strategy to fuzzify the existing antecedents in r the following way:
		
			4.1 Examine all possible support bounds and select the one which gains the highest purity on the training data.
			4.2 Set the maximum support bound determined in 4.1 and restart with 4.1 but withouth the fuzzified antecedent.
		ENDDO
		
		X.1. Pruning:
		Incrementally prune each rule and allow the pruning of any final sequences of the antecedents;The pruning metric is (p-n)/(p+n) -- but it's actually 2p/(p+n) -1, so in this implementation we simply use p/(p+n) (actually (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).
		
		Classification time:
		If an instance is not covered by any rule, apply a rule stretching mechanism: Cut every rule just in front of the first discriminating antecedent such that the this way stretched rule covers the instance. 
		Doing this for all rules will lead to a set of rules in which each one covers the instance (or is empty and may be excluded). To determine the rule that assigns the class calculate the weight given by its purity using the m-measure on the one hand the Laplace-fraction of antecedents left in comparison to the original number of the respective rule. 
		The rule that maximizes that value is from the class that will be assigned.
		</howWork>

		<parameterSpec>  
			<param>Number of optimizations: Times that the rule optimization process is carried out after learning the initial rule set</param>
			<param>Number of folds: Set number of folds for REP (One fold is used as pruning set).</param>
		</parameterSpec>

		<properties>
			<continuous>Yes</continuous>
			<discretized>Yes</discretized>
			<integer>Yes</integer>
			<nominal>Yes</nominal>
			<valueLess>No</valueLess>
			<impreciseValue>No</impreciseValue>
		</properties>

	</generalDescription>
<example>Not available</example>

</method>
